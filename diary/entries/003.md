<!--- metadata

title: You can't trust digital pictures, voice & video anymore
date: 2025-05-04
slug: 003
id: 003
week:  Week 19
summary: I saw a video about Crowdstrike's 2024 Global Threat report and one attack vector that increased over 400% was voice phishing. This stand out to me as this was a topic I had talked and warned my parents about so I felt like rising development in AI you can't trust anything digital anymore. So the first question I was thinking was, how do you fix that?
tags: [ "Authentication", "Encryption", "Watermarking" ]

--->

## Focus of the week

I saw a video about Crowdstrike's 2024 Global Threat report and one attack vector that increased over 400% was voice phishing. This stand out to me as this was a topic I had talked and warned my parents about so I felt like rising development in AI you can't trust anything digital anymore. So the first question I was thinking was, how do you fix that?

I started to do some research using the new ChatGPT:s DeepResearch method and found out this:

*Everything from here is ChatGPT generated using the DeepResearch method.*

# Authenticating Media Content: Provenance and Liveness Verification

**Content Provenance (C2PA)** - The Coalition for Content Provenance and Authenticity (C2PA) defines an open standard for **content credentials**: cryptographically signed metadata that travels with an image, video, or audio file. Every pixel (or frame) is hashed and tied to statements of origin (camera model, author identity, edit history, etc.)​. In effect, a C2PA-signed file carries a “nutrition label” of its creation chain. For example, a Digimarc browser extension uses C2PA to flag two similar snowy landscape images – one shot by a camera, one AI-generated – as having different provenance​.

 As the Linux Foundation notes, C2PA’s Content Credentials enable tracing “the path of different types of media, from production… to consumption”​. Major companies (Adobe, Google, Microsoft, Sony, Truepic, etc.) and camera makers (Canon, Nikon, Leica, Sony) back C2PA. Current tools include the C2PA consortium’s open-source SDKs (Rust, JavaScript, Python, C/C++)​ and Digimarc’s C2PA manifest-validator Chrome extension​.

 On the platform side, TikTok now reads C2PA credentials on uploads from other apps and plans to embed them on TikTok videos​, whereas YouTube’s policy today relies on creators self-labeling AI content​.

- **How it works:** C2PA binds cryptographic hashes to the media content and signs provenance claims. A verifier can recalc hashes and compare them to the manifest. As described by C2PA’s documentation, it “uses cryptography to encode details about the origins of a piece of content, by using cryptographic hashes to bind to every pixel… Similar to a nutrition label, C2PA provides information about who created the content and how, when, and where it was created”. This protects integrity: any edit will break the hash chain.

- **Integration:** Some smartphone and camera manufacturers are adding C2PA support. (E.g., Leica’s SL3-S camera and Adobe Firefly tag images with credentials.) Editing tools like Adobe Photoshop/Lightroom can attach edit history into credentials. Services like Truepic use C2PA end-to-end to certify uploads.

- **Open-source tools:** The C2PA GitHub hosts SDKs for authoring and validating credentials (e.g. `c2pa-rs`, `c2pa-js`, `c2pa-python`). Digimarc’s extension (open source) scans web images and displays their C2PA “Content Credentials” if present​.

| **Approach**                      | **What It Does**                                | **Advantages**                                                  | **Limitations**                                                                                      | **Examples/Tools**                                                          |
| --------------------------------- | ----------------------------------------------- | --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| **Cryptographic Hash/Blockchain** | Hash of content stored externally (ledger)      | Strong tamper detection; works across re-encodings              | Requires external database/ledger lookup; no inline marker; does not survive content copying         | Storing image hashes on a blockchain (Mediachain), C2PA manifests in ledger |
| **Metadata Tagging**              | Storing info in file headers (EXIF/XMP)         | Simple to implement; preserves edit info in text form           | Stripped by many platforms (social media, email, resizing) easy to forge or remove                   | EXIF metadata (camera, timestamps), XMP tags, IPTC fields                   |
| **Visible Watermark**             | Overt overlay text/logo on media                | Easy for humans to spot; no special readers needed              | Easily removed or cropped; degrades user experience; bypassable                                      | Copyright logos on images or videos (stock photos, TV broadcasts)           |
| **Invisible (Digital) Watermark** | Hidden pattern in image/audio (DCT/DWT/LSB)     | Stealthy; can survive some transformations (e.g. compression)   | Removable by strong editing or adversarial methods; requires detector; decoding errors possible      | Digimarc watermarking; open libraries (e.g. Python `invisible-watermark`)    |
| **C2PA Content Credentials**      | Crypto-signed provenance metadata (image/video) | Tamper-evident, cryptographically strong, globally standardized | Requires end-to-end support (cameras, editors, platforms); metadata can be stripped by some services | C2PA SDKs (Rust/JS/Python), Adobe tools, TikTok, Digimarc                   |

## Watermarking Techniques

Various watermarking methods complement or substitute cryptographic provenance.

- **Visible Watermarks:** These are human-readable marks (logos, text) overlaid on content. They are trivial to see, but as a Mozilla study notes, *“human-facing”* labels are easily bypassed and yield low effectiveness​. An attacker can crop, blur, or paint over a visible watermark. Visible marks can deter casual misuse (e.g. TV broadcasts, stock photos) but provide no strong security.

- **Invisible Watermarks (Digital Steganography):** Information is embedded into the media signal (often in the frequency or spatial domain). For example, one can modify DCT coefficients in a JPEG or use wavelet transforms to encode a binary watermark. Such marks can be robust (survive compression, minor edits) or fragile (break on any change). Research emphasizes that imperceptible watermarking *“incorporates unchangeable identifiers into multimedia files”* for authenticity. Mozilla’s analysis found machine-readable watermarks are “less likely to be removed or altered” than visible marks​. However, adversarial attacks exist: recent studies show text/image watermarks are often removable by intelligent paraphrasing or filtering​, and even the best current audio watermarks are “very vulnerable” to spoofing.

- **Metadata Watermarking:** Embedding provenance in file metadata (EXIF, XMP, etc.) is easy and preserves quality, but it is fragile. Many platforms strip metadata on upload​, so this approach is insufficient alone.

- **Cryptographic/Blockchain Hashing:** Another strategy is to compute a secure hash of the media and store it (for example on a blockchain). This makes any change (even one pixel) obvious: during verification, a new hash mismatch flags tampering. This method does not alter the media, but relies on external storage. It also requires that the original hash database is trustworthy and widely checked.

Overall, invisible and cryptographic techniques are more robust than visible labels, but no watermark is foolproof. Even so, they can be useful when combined with detection tools.

## Live Human Authentication

While provenance labels content, **liveness detection** assures a human (not a photo or AI) is present when capturing video or audio. Emerging approaches include:

- **Face/Video Liveness:** Modern systems use a mix of behavioral and technical cues. Common checks include asking users to blink, smile, or move their head, and verifying these motions with a face model. For example, a face anti-spoofing SDK might analyze blinking patterns and facial micro-expressions to ensure the subject is live​. More advanced techniques use 3D modeling or depth sensors: e.g. active IR illumination or stereo cameras to confirm 3D geometry (as in smartphone face-ID). These methods are effective against simple photo or video replays but require either active participation or special hardware. Standalone face liveness APIs (e.g. MediaPipe FaceMesh, InsightFace) can be adapted to trigger on natural facial dynamics.

- **Voice Anti-Spoofing:** Audio liveness often leverages random phrase challenges or acoustic analysis. For instance, voice authentication systems may prompt the speaker to repeat a random sentence, ensuring the response is synthesized in real time. More subtle methods analyze the audio signal: neural networks can detect artifacts of generated speech or conversions. The ASVspoof community (challenge since 2015) provides datasets and baselines for such detection​. Research prototypes even compare how sound waves travel from mouth to microphone: for example, “LiveEar” detects differences in sound arrival dynamics between real human speech and playback attacks​. Others use two-factor approaches (e.g. measuring bone-conducted vibrations vs microphone input​). However, state-of-the-art voice-cloning remains a challenge; current solutions catch basic attacks but can be fooled by high-quality generative speech.

- **Multi-modal & Sensor Fusion:** Combining modalities (face+voice, or adding biometrics) increases assurance. For example, synchronizing lip movements with audio can catch dubbed video deepfakes, and combining fingerprint or heartbeat sensors with facial capture further raises the bar. Commercial identity-verification APIs (Aware, Jumio, etc.) implement multi-factor liveness, but these are usually closed solutions.

| **Method**                   | **Type**     | **Description**                                                              | **Strengths**                                                   | **Limitations**                                                                            |
| ---------------------------- | ------------ | ---------------------------------------------------------------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| **Multi-modal Verification** | Audio+Video  | Cross-checks two modalities (e.g. lip-sync, simultaneous face/voice).        | Harder to fake both correctly; higher confidence.               | More complex to implement; requires multiple sensors (camera+mic).                         |
| **Depth/IR Face Liveness**   | Video/Depth  | Uses depth sensors or IR to capture 3D facial structure or thermal patterns. | Very robust to 2D replay or masks.                              | Requires specialized hardware (not in typical webcams).                                    |
| **Voice Anti-Spoof**         | Audio/Mic    | Analyzes acoustic features or issues spoken challenges.                      | Uses existing microphone; can leverage ASVspoof-trained models. | Synthetic voices are improving; background noise and recording quality impact performance. |
| **Face Liveness (2D)**       | Video/Camera | Detects eye-blinks, expressions, head movements to confirm a live user.      | Works with any camera; intuitive.                               | Can be spoofed by video, phone displays, or high-res photos.                               |

## Open-Source Tools & Frameworks

Building a prototype MVP can leverage many available libraries:

- **C2PA SDKs:** Official libraries (`c2pa-rs`, `c2pa-js`, `c2pa-python` etc.) let you embed and verify content credentials​. (E.g. Python binding wraps the Rust core.)

- **Browser Extension:** Digimarc’s C2PA Chrome extension (open-source) demonstrates verifying signed metadata on web images/videos​.

- **Watermark Libraries:** The Python package invisible-watermark uses DWT+DCT to embed/extract image watermarks. OpenCV or Pillow can add visible logos.

- **Deepfake/Anti-Spoof Models:** Open research code (e.g. FaceForensics++, Kaggle models, ASVspoof baselines) can be integrated via TensorFlow/PyTorch. For audio, toolkits like SpeechBrain or the ASVspoof challenge code can provide pre-built anti-spoofing models.

- **Face & Vision APIs:** Libraries like OpenCV, dlib, or Google’s MediaPipe handle real-time face detection and landmark tracking (for blink detection, liveness analysis). Commercial SDKs (e.g. KBY Face Liveness, Aware) also exist for rapid testing.

- **Voice Processing:** openSMILE or librosa for feature extraction, combined with machine-learning classifiers, enable voice spoof detection. ASVspoof datasets (e.g. LA/DF tracks) are publicly available to train models.

Combining these, one could build an MVP as follows: attach a C2PA credential to images at capture (using the Python SDK), then verify it at upload. Simultaneously, use a face-detection library (MediaPipe/OpenCV) to check for real-time user motions (blinks or micro-expressions). For voice, one might apply a pretrained anti-spoofing classifier on recorded speech. Finally, any uploaded content could be watermarked invisibly (using the library above) or have its hash logged to a simple blockchain to detect any later changes.

## Barriers to Enterprise Adoption

Despite promising technology, adoption faces hurdles:

- **Ecosystem Immaturity:** The infrastructure for content credentials is still young​. Few consumer devices fully support C2PA today: major camera makers announced support but only a handful of models have shipped​. For example, Samsung’s new phone will label only AI-edited photos, and Apple has no announced plan​. Editing software support is likewise limited mainly to Adobe’s apps​. Until cameras, platforms, and apps uniformly integrate these standards, coverage will be patchy.

- **Data Stripping & Compatibility:** Many platforms (social media, messaging apps) automatically compress images and strip metadata. Mozilla notes that metadata watermarks can be easily removed by common workflows​. C2PA addresses this by embedding manifests in file streams, but conversion to unsupported formats still breaks signatures.

- **Reliance on User Compliance:** Some platforms prefer policy-based labels over technical methods. For instance, YouTube now depends on creators to **self-report** synthetic videos with content-disclosure checkboxes​. This can be easily ignored or abused. In contrast, TikTok’s strategy of auto-reading C2PA tags shows a move towards machine-verifiable proofs​. A mixed approach can confuse users and dilute trust.

- **Reliability and Resilience:** Specialists caution that watermarking and detection are an ongoing battle​. If adversaries succeed in faking or erasing markers, public confidence will diminish. One evaluation starkly notes that if fake content creators manage to trick a detector into tagging genuine content as “AI-generated,” it would “entirely undermine public confidence in watermarking”. Therefore, any system must minimize false positives to maintain credibility.

- **Performance and Cost:** Verifying cryptographic proofs and running ML detectors at scale (billions of uploads) requires compute and careful architecture. Enterprises worry about the throughput and latency overhead. Also, visible watermarks may hurt user engagement (due to aesthetic impact).

- **Regulatory & Privacy Concerns:** Mandating provenance can conflict with privacy (e.g. revealing capture location or creator identity). Global regulations on AI content are still evolving; inconsistent laws make platform policies complex.

In summary, while technical solutions exist (C2PA, watermarks, liveness checks), **enterprise adoption** depends on ecosystem buy-in and usability. Enterprises like TikTok or Adobe see strategic value in content credentials and are moving first​. Others rely on moderation policies for now. For a prototype tool or MVP, a promising start is to leverage C2PA libraries and open liveness detectors (face blink checks, voice anti-spoof models) as building blocks. Combining multiple orthogonal signals (provenance plus liveness plus watermark) will yield the most robust authenticity verification in practice.

## How I Felt

This week, even tho I had a real plan and I felt like I learned something at least, it seems that I only stumble upon things that are to complicated for me to understand or then are not viable. This has been little bit frustrating, but I am trying to work tough it.

### Sources

Crowdstrike Global Threat Report

<https://www.crowdstrike.com/en-us/global-threat-report/>

How C2PA Helps Combat Misleading Information

<https://www.linuxfoundation.org/blog/how-c2pa-helps-combat-misleading-information>

GitHub - digimarc-corp/c2pa-content-credentials-extension: C2PA Chrome Extension

<https://github.com/digimarc-corp/c2pa-content-credentials-extension>

Content Credentials Show Promise, but Ecosystem Still Young

<https://www.darkreading.com/mobile-security/content-credentials-show-promise-but-ecosystem-still-young>

Content Authenticity Initiative · GitHub

<https://github.com/contentauth>

Partnering with our industry to advance AI transparency and literacy - Newsroom | TikTok

<https://newsroom.tiktok.com/en-us/partnering-with-our-industry-to-advance-ai-transparency-and-literacy>

Our approach to responsible AI innovation - YouTube Blog

<https://blog.youtube/inside-youtube/our-approach-to-responsible-ai-innovation/>

In Transparency We Trust? - Mozilla Foundation

<https://foundation.mozilla.org/en/research/library/in-transparency-we-trust/research-report/>

A survey on multimedia-enabled deepfake detection: state-of-the-art tools and techniques, emerging trends, current challenges & limitations, and future directions | Discover Computing

<https://link.springer.com/article/10.1007/s10791-025-09550-0>

On-Device Watermarking: A Socio-Technical Imperative for Authenticity in the Age of Generative AI

<https://arxiv.org/html/2504.13205v1>

GitHub - ShieldMnt/invisible-watermark: python library for invisible image watermark (blind image watermark)

<https://github.com/ShieldMnt/invisible-watermark>

A survey on multimedia-enabled deepfake detection: state-of-the-art tools and techniques, emerging trends, current challenges & limitations, and future directions | Discover Computing

<https://link.springer.com/article/10.1007/s10791-025-09550-0>

Ultimate Guide to The Best Face Anti-Spoofing Methods 2025 - KBY-AI

<https://kby-ai.com/face-anti-spoofing-methods/>

Audio Deepfake Detection: What Has Been Achieved and What Lies Ahead - PMC

<https://pmc.ncbi.nlm.nih.gov/articles/PMC11991371/>

Voice liveness detection: methods overview — Antispoofing Wiki

<https://antispoofing.org/voice-liveness-detection-systems-challenges-and-solutions/>

Content Credentials Show Promise, but Ecosystem Still Young

<https://www.darkreading.com/mobile-security/content-credentials-show-promise-but-ecosystem-still-young>

On-Device Watermarking: A Socio-Technical Imperative for Authenticity in the Age of Generative AI

<https://arxiv.org/html/2504.13205v1>
